{
    "collab_server" : "",
    "contents" : "# Heuristic Analysis \n\n## Evaluation Functions\n```python \n    def open_moves(myMoves):\n        \"\"\"\n        Objective: All available moves; given from open_move_score() in the example.\n\n        Reasoning: N/A.\n        \"\"\"\n\n    def difference_moves(myMoves, opponentMoves):\n        \"\"\"\n        Objective: Difference between your moves and the opponent moves; given from improved_score() in the example.\n\n        Reasoning: N/A.\n        \"\"\"\n\n    def quadratic_difference_moves(myMoves, opponentMoves):\n        \"\"\"\n        Objective: Difference between your moves and opponent moves, squared. \n\n        Reasoning: The difference_moves() function does not adequately acknowledge negative situations (when the opponent has more moves than you), so perhaps squaring might help. \n        \"\"\"\n\n    def weighted_difference_moves(myMoves, opponentMoves, weight):\n        \"\"\"\n        Objective: Weighting the number of by a factor to make the value more dangerous. \n\n        Reasoning: It was hinted at during the lectures, but perhaps penalizing the opponent by weighting their moves more heavily might make us more reactive to any changes in their position.\n        \"\"\"\n\n    def avoid_edges(myMoves, opponentMoves):\n        \"\"\"\n        Objective: Attemps to make the center of the board less valuable than the edges.  \n\n        Reasoning: A personal experiment to see whether the edges of the board were significantly more valuable than the center, and penalizing moves that favored that center. Turned out to not perform very well.\n        \"\"\"\n\n    def final_score(heur1_weight, heur2_weight, heur3_weight, heur4_weight, heur5_weight):\n        \"\"\"\n        Objective: Weights all evaluation function by certain values to yield a final score.\n\n        Reasoning: Easily my favorite heuristic here; why have just one when you can have a weighted average of all the positions, and allow multiple factors to inform your final score? \n        \"\"\"\n\n```\n\n## Testing\n\nAll the functions were developed and tested with `tournament.py`, and yielded the following results:\n\n### Test 1: `final_score(0.4, 0.3, 0.3)`\nAttemps quasi-equal weights with the three original functions. \n\n```\n    # ID_Improved         65.00%\n    # Student             57.86%\n```\nSo far, performing worse than the test agent. Still, nice to see the weighted average thing might actually work. Perhaps if we try the new quadratic variant? \n\n### Test 2: `final_score(0.1, 0.1, 0.8)`\nThis one favours the quadratic variant much more than the first two.\n```\n    # ID_Improved         52.14%\n    # Student             54.29%\n```\nIt seems squaring is useful, but only marginally. Now let's try some new heuristics. \n\n### Test 3: `final_score(0, 0, 0, 1)`\nThis weighting favours the fourth heuristic, which attemps to penalize the opponents number of moves.\n```\n    # ID_Improved         52.14%\n    # Student             52.86%\n```\nIt did sub par. Maybe if we incorporate the old ones again? \n\n### Test 4: `final_score(0.25, 0.25, 0.25, 0.25)`\nThis one favours all four available heuristics. \n```\n    # ID_Improved         59.29%\n    # Student             66.43%\n``` \nAlright, weighting them all was worth it! I am very happy with the weighted average heuristic, but let's try building another. \n\n### Test 5: `final_score(0, 0, 0, 0, 1)`\nThis one favours the fifth heuristic. Spoiler alert: it doesn't do very well, but luckily the tests discovered that pretty quickly: \n```\n    # ID_Improved         57.86%\n    # Student             47.14%\n```\n### Test 6: `final_score(0.2, 0.2, 0.2, 0.2, 0.2)`\nThis one goes back to the old approach of favouring all the functions; what happens if we mix it in with the other evaluation functions? \n```\n    # ID_Improved         71.43%\n    # Student             47.14%\n```\nWow, that's pretty rough.  \n\n### Test 7: `final_score(0.3, 0.3, 0.25, 0.15, 0.0)`\nLet's drop it for a new system favouring our original heuristics, instead of this new experiments. \n```\n    # ID_Improved         61.14%\n    # Student             66.43%\n```\nSo it seems the heuristics we've deployed so far have worked, but my last experiment was a bust. If I were to continue, I would probably revert to Test 4 and iterate from that if I were to continue. ",
    "created" : 1489758454540.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1240544000",
    "id" : "E28BC07",
    "lastKnownWriteTime" : 1489700363,
    "last_content_update" : 1489700363,
    "path" : "~/Dropbox/udacity/AIND/isolationAI/heuristic_analysis.md",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 9,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "markdown"
}